\documentclass[english, 12pt]{article}
\usepackage{notes}
% Uncomment these for a different family of fonts
% \usepackage{cmbright}
% \renewcommand{\sfdefault}{cmss}
% \renewcommand{\familydefault}{\sfdefault}
\usepackage{euler}
\usepackage{concmath}
\usepackage{soul}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
 
\urlstyle{same}

\newcommand{\thiscoursecode}{CMPS132}
\newcommand{\thiscoursename}{Computability and Complexity Theory}
\newcommand{\thisprof}{Dr. Seshadhri Comandur}
\newcommand{\me}{Andrew Edwards}
\newcommand{\thisterm}{(Spring) 2015}
\newcommand{\website}{https://users.soe.ucsc.edu/~sesh/cmps132-15/cmps132-15.html}

% Headers
\chead{\thiscoursecode}
\lhead{\thisterm}


%%%%%% TITLE %%%%%%
\newcommand{\notefront} {
\pagenumbering{roman}
\begin{center}

{\ttfamily \url{\website}} {\small}

\textbf{\Huge{\noun{\thiscoursecode}}}{\Huge \par}

{\large{\noun{\thiscoursename}}}\\ \vspace{0.1in}

{Notes by: {\noun \me}} \\\vspace{0.1in}

  {\noun \thisprof} \ $\bullet$ \ {\noun \thisterm} \ $\bullet$ \ {\noun {UC Santa Cruz}} \\

  \end{center}
  }

% Begin Document
\begin{document}

  % Notes front
  \notefront
  % Table of Contents and List of Figures
  \tocandfigures
  % Abstract
  \doabstract{TODO}

  \section{Introduction}
  \subsection{What is Computation?}
  The various disciplines of computer science can roughly be split into
  those focusing primarily on application or and those focusing on theory. 
  We see the same division in most sciences: theoretical versus experimental
  physics, theories of molecular biology and laboratory experiments,
  pure and applied mathematics, etc. However, in computer science,
  the two sides are more closely intertwined, and the relationship between
  theory and application has a unique and far-reaching feature. \n

  Theoretical computer science research puts physical limitations on what
  is acheivable by computation, and hence by computers. This top-down
  causality is peculiar and noteworthy. In physics, a theory must be 
  supported by experimentation in order to be accepted, and more often than
  not, the theory is physically motivated from the outset. Biology is an even
  more extreme example of scientific law guided by physical experiment. 
  As we will see, the physical ability to harness the power of computation
  is constrained by a purely mathematical result. \n

  But what exactly do we mean by computation? \n

  For now, we appeal to intuition. Computation is the process of taking a
  set of {\bf inputs} to an {\bf output}, following some well-defined 
  sequence of steps. The description of these steps is called an {\bf algorithm}. \n

  Computation is the mechanism of transforming information, via some specific
  set of rules. Hitherto we have not defined {\it what form} this set
  of rules may take, nor have we even defined what form the input can take.
  For now, we simply assume that the input is {\bf finite} and that the
  description for how the computation proceeds is also finite. \n


  At this point, a natural question is, ``What are the limits of computation?'' 
  \subsection{Hilbert's 23 Problems and Hilbert's Program}
  David Hilbert was a prominent mathematician of the late nineteenth and
  early twentieth century. His research areas included functional analysis,
  geometry, and mathematical logic. Hilbert also was one of the first
  mathematicians to distinguish between mathematics and metmathematics.
  \footnote{\url{http://plato.stanford.edu/entries/hilbert-program/}} \n

  In 1900, Hilbert presented a list of 23 unsolved problems at the
  International Congress of Mathematicians in Paris. The tenth problem
  of the list is,
  \begin{quote}
      Find an algorithm to determine whether a given polynomial Diophantine 
      equation with integer coefficients has an integer solution, or prove
      that no such algorithm exists.
  \end{quote}
  In general, a Diophantine equation is one whose sought after solutions must
  be integers. For example, consider
  \[ x^n + y^n = z^n \]
  If $n = 1$, then there are infinitely many solutions. The set 
  \[ \{(x, y, z) = (p, q, p + q) : p, q \in \mathbb{Z}\} \] gives a
  parametrization of all solutions. If $n = 2$, we reach the familiar
  Pythagorean equation. We can parametrize all solutions by the set
  \footnote{See Chapter 0 of Allen Hatcher's {\it 
  \href{http://www.math.cornell.edu/~hatcher/TN/TNpage.html}
       {Topology of Numbers} } for a great exposition of this result.}
  \[ \{ (x, y, z) = (p^2 - q^2, 2pq, p^2 + q^2) : p, q \in \mathbb{Z}, p > q
  \}. \]
  However, for all $n > 2$, this Diophantine equation has no solutions. This
  fact is known as Fermat's Last Theorem, and took mathematicians centuries
  to prove (it was finally proven by Andrew Wiles in 1994). Similarly, the
  (somewhat trivial) example \(x^2 + 1 = 0\) has no integer solutions. So 
  clearly some Diophantine equations have solutions and some do not.
  Hilbert's tenth problem asks for an algorithm that takes as input a 
  Diophantine equation an returns (say) ``TRUE'' if a solution exists and
  ``FALSE'' otherwise. \n

  It turns out that no such algorithm exists. Over the course of 21 years,
  mathematicians Yuri Matiyasevich, Martin Davis, Julia Robins, and Hilary
  Putnam worked on the problem, and eventually produced the MDRP theorem,
  which (essentially) proves that no such algorithm exists. Their proof 
  draws an equivalence between the set of solutions to a given Diophantine
  equiation and computably enumerable sets (a concept we will study later).
  Their proof involves very sophisticated mathematics, so for now we will
  have to trust the mathematical community on the validity of this result.
  \n

  Knowing that Hilbert's tenth problem cannot be solved by an algorithm,
  we have our first theorem:
  \begin{thrm}
      There exist problems unsolveable by computation.
  \end{thrm}
  We will not be able to prove this result just yet; we will come back to
  it in Chapter 2. However, given a specific model of computation, it's to
  not too hard to concoct an unsolveable problem.
  \subsection{Computing Functions}
  Consider the following set of functions
  \[ \mathcal{F} = \{ f | f : \mathbb{N} \rightarrow \{0,1\} \} \]
  In English, $\mathcal{F}$ is the set of all functions mapping each natural
  numbers \(\mathbb{N} = \{0, 1, \dots \} \) to either zero or one.
  Now, suppose we pick a specific function $f \in \mathcal{F}$. Can we
  compute $f$? \n

  Clearly the answer depends on how we do the computing! As we saw in
  CMPS130, a Pushdown Automaton (PDA) is strictly more powerful than a
  Deterministic Finite Automaton (DFA), and a Turing Machine (TM) is
  strictly more powerful than a PDA. For now, let's not be too concerned
  with formality. Let's pick a familiar, powerful model of computation:
  $\pC$ programs. \n

  As an example, consider the function 
  \[f(n) = \begin{cases} 0 & \text{if $n$ is even} \\
                         1 & \text{if $n$ is odd} \end{cases}\]
  The following $\pC$ function, which we'll call \texttt{f.c}.
  \begin{lstlisting}
  #include <stdio.h>
  int main(int argc, char** argv) {
      int n = argc - 1;
      printf(``%d'', n % 2);
  }
  \end{lstlisting}
  Here is the result of compiling and running this program on a Unix machine:
  \begin{lstlisting}[language=bash]
  $ gcc f.c -o f
  $ ./f
    0
  $ ./f 1
    1
  $ ./f 1 1
    0
  $ ./f 1 1 1
    1
  \end{lstlisting}
  So our program $\texttt{f.c}$ can compute $f(n)$ by passing in 
  $n$ space separated arguments to the compiled program. To keep it simple we
  can use a space separated unary encoding of $n$. We can alter this program
  in a virtually unlimited number of ways to compute other functions, such
  as $g(n) = 1$ if $n$ is odd, $h(n) = 1$ if $n$ is greater than $42$, and 
  $p(n) = 1$ if $n$ is a prime number.\n

  Does every function $f \in \mathcal{F}$ have a corresponding program 
  \texttt{f.c}? We are about to see that the answer is no, which leads us
  to
  \begin{thrm} There exists a function $f \in \mathcal{F}$ such that no
               $\pC$ program computes $f$.
  \end{thrm}
  Note the difference between Theorem 1.1 and Theorem 1.2. The first claims
  that there exist problems that {\bf no} model of computation can solve. The
  second makes a weaker claim: given a specific model of computation ($\pC$ 
  programs), there exist problems that cannot be solved by the model.

  The proof is not too hard, and it takes advantage of the following simple
  fact:\[\text{ 
  {\it Every} $\pC$ {\it program is a finite string of characters}.
  }\]
  We now prove Theorem 1.2.
  \begin{proof}
  Imagine listing {all finite character strings} (say ASCII strings to 
  keep things simple). Most of these won't correspond to a $\pC$ program,
  but some of them do. We can imagine stepping through all the character 
  strings of length 1, all of length 2, ad infinitum, and trying to compile
  the string with \texttt{gcc}. We keep all programs that compile and,
  when run via 
  \texttt{./f} $\underbrace{\texttt{1 1}\dots\texttt{1}}_{n}$, print out
  zero or one. We can just discard those that don't.\footnote{If you are worried
  about the program printing stuff other than zero or one, we can change our model 
  to only allow \texttt{printf(``0'')} and \texttt{printf(``1'')} print statements. Or
  we could interpret any character that is not \texttt{0} to correspond to a \texttt{1}.
  Our intention is not to be overly technical or pedantic at this point.} \n

  We will eventually list all valid programs in our model. Let's label
  the program as \texttt{f\_0.c}, \texttt{f\_1.c}, \dots For each program
  \texttt{f\_i.c}, let $f_i(n)$ be the funciton this program computes.
  We can view all the functions and their outputs with a rectangular grid:
\[
\begin{tabular}{c|c|c|c|c}
$n$             & $0$   & $1$   & $2$   & \dots \\
\hline
$f_0(n)$ & $0$   & $0$   & $0$   & \dots \\
\hline
$f_1(n)$ & $1$   & $0$   & $1$   & \dots \\
\hline
$f_2(n)$ & $1$ & $1$ & $0$ & \dots \\
\hline
\vdots & \vdots & \vdots & \vdots & $\ddots$
\end{tabular}
\]
  Now, consider the following function:
  \[ f(n) = 1 - f_n(n) \]
  This is a well defined function. Given $n$, we can find the $n^{th}$ $\pC$
  program \texttt{f\_n.c} and compute $f_n(n)$. Then $1 - f_n(n)$ gives us $f(n)$.\n

  The question is, does $f$ appear in our list?\n

  This is the crux of the proof: $f$ {\it does not} appear in the list. 
  It disagrees with every function in the list on where to map the input
  $n$. But this list contains all functions computable by $\pC$ programs! 
  Therefore, $f$ is not computable.\n

  This proves Theorem 1.2.
  \end{proof}
  The argument above is called a {\bf diagonal argument}, and its origins
  trace back to the German mathematician Georg Cantor. In 1891 he proved
  that there is no bijection between the set $T$ of all infinite binary 
  sequences and the set of natural numbers. Following this style of reasoning,
  one can show that there is no bijection between the set of real numbers
  and the set of natural numbers, and that there is no bijection between the
  natural numbers and the set of all subsets of natural numbers. \n

  \subsection{A Look Ahead}
  TODO

  \section{Computational Models}
  In this chapter we will explore three models of computation. The first
  we will call $\pC$ programs. In reality, we will only need a very 
  limited subset of the language, which is the first surprise of computability
  theory:
  \[ \textbf{1. }\textit{Few rules give rise to all computation.} \]
  The second model is the Turing Machine. We assume the reader has seen
  Turing Machines before, but we will review the notation and the semantics.
  The third model is called $\mu$-Recursive functions. If you have ever
  programmed in a functional programming language, then this formalism will
  be familiar. Unlike $\pC$ programs and Turing Machines, $\mu$-Recursive
  functions operate exclusively on natural numbers. \n

  On the surface, these three models seem to be very different. This brings
  us to the second surprise of computability theory:
  \[ \textbf{2. } \textit{All models of computation are equivalent.} \]
  
  \subsection{``\pC\,'' Programs}
  The first surprise of computability theory is that only a few rules
  are required to acheive the full power of computation. We demonstrate
  that in this chapter. \n

  The syntax of our first model is very simple. All programs have the 
  following form: 
  \begin{itemize}
      \item Each take a fixed number of inputs $x_1, x_2, \dots, x_k$,
            give a single output $y$, and have access to a fixed number
            of temporary variables $z_1, z_2, \dots, z_n$. 
      \item All input, ouput, and temporary variables are natural numbers.
      \item The following two operations are valid on any variables:
          \begin{enumerate}
              \item $\inc{v}$, which increments the variable $v$.
              \item $\dec{v}$, which decrements the variable $v$, except at zero, 
                whence $\inc{v})_{v=0} = 0$.
          \end{enumerate}
      \item The description of a program is a sequence of instructions,
            which is either an operation on a variable or the following
            conditional branch:
            \[ \text{If } v = 0 \text{, then go to instruction } n
            \text{. Else, continue.} \]
  \end{itemize}
  Here is an example program,
  \begin{lstlisting}
  
  \end{lstlisting}
  \subsection{Turing Machines}
  \subsection{$\mu$-Recursive Functions}
  \subsection{Church-Turing Thesis}
  \end{document}
